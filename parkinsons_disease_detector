#importing the necessary libraries
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
import shap
import umap.umap_ as umap
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (Dense, Dropout, LSTM, Bidirectional, Conv1D, MaxPooling1D, GlobalAveragePooling1D, BatchNormalization, Reshape)
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc
from sklearn.manifold import TSNE

# Load the parkinson's disease dataset
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data"
df = pd.read_csv(url)

# Droppin the non-feature columns
X = df.drop(columns=['name', 'status'])  # 'status' is the target variable
y = df['status']  # 1 = Parkinson’s, 0 = Healthy

# Visualize feature distribution using KDE plots
plt.figure(figsize=(12, 6))
sns.kdeplot(df[df['status'] == 1]['PPE'], label='Parkinson’s', shade=True)
sns.kdeplot(df[df['status'] == 0]['PPE'], label='Healthy', shade=True)
plt.title('Feature Distribution: PPE')
plt.legend()
plt.show()

# Splitting the  dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Normalize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Reshape input for CNN-LSTM model
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

# Define hyperparameters manually
filters_1 = 64
filters_2 = 32
kernel_size = 3
dropout_1 = 0.3
dropout_2 = 0.3
lstm_units_1 = 64
lstm_units_2 = 32
dense_units = 32
learning_rate = 0.0002
l2_reg = 0.001

# Building CNN-LSTM Model
model = Sequential()
model.add(Conv1D(filters=filters_1, kernel_size=kernel_size, activation='relu',
                  kernel_regularizer=l2(l2_reg), input_shape=(X_train.shape[1], 1)))
model.add(BatchNormalization())
model.add(MaxPooling1D(pool_size=2))
model.add(Dropout(dropout_1))

model.add(Conv1D(filters=filters_2, kernel_size=kernel_size, activation='relu',
                  kernel_regularizer=l2(l2_reg)))
model.add(BatchNormalization())
model.add(GlobalAveragePooling1D())

model.add(Reshape((1, filters_2)))  # Reshape before LSTM
model.add(Bidirectional(LSTM(lstm_units_1, return_sequences=True, kernel_regularizer=l2(l2_reg))))
model.add(Dropout(dropout_2))

model.add(Bidirectional(LSTM(lstm_units_2, kernel_regularizer=l2(l2_reg))))
model.add(Dropout(dropout_2))

model.add(Dense(dense_units, activation='relu', kernel_regularizer=l2(l2_reg)))
model.add(BatchNormalization())
model.add(Dropout(dropout_2))

model.add(Dense(1, activation='sigmoid'))

# Compile the Model
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

# Training the model
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test),
                    callbacks=[ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1),
                               EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)])

# Save the final model
model.save("manual_tuned_parkinsons_model.h5")

# Predictions
y_pred_prob = model.predict(X_test)
y_pred = (y_pred_prob > 0.5).astype(int)

# Evaluation of model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

# Confusion matrix
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

# UMAP Projection
umap_model = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='euclidean')
X_umap = umap_model.fit_transform(X_test.reshape(X_test.shape[0], -1))
plt.figure(figsize=(8, 6))
sns.scatterplot(x=X_umap[:, 0], y=X_umap[:, 1], hue=y_test, palette='viridis')
plt.title('UMAP Projection of Features')
plt.legend()
plt.show()

# Improved ROC Curve with Thresholds
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(8, 5))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
for i in range(0, len(thresholds), 10):
    plt.annotate(f'{thresholds[i]:.2f}', (fpr[i], tpr[i]))
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Receiver Operating Characteristic (ROC) Curve")
plt.legend()
plt.show()


